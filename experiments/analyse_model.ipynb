{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 20:51:14.393824: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-20 20:51:14.542508: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-20 20:51:15.143170: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-11.4/lib64\n",
      "2022-12-20 20:51:15.143273: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-11.4/lib64\n",
      "2022-12-20 20:51:15.143281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import autokeras as ak\n",
    "#load data\n",
    "from SB_Test_runner import get_scens_per_dim, get_simulated_data\n",
    "\n",
    "class newmodel(MLPClassifier):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def predict(self, X):\n",
    "        y = self.model.predict(X)\n",
    "        return np.argmax(y, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings for this experiment\n",
    "\n",
    "20000 repetitions per class (group of scenes) and 500 samples (runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings for this experiment\n",
    "rep = 200\n",
    "n_samples = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unif': 800, 'centre': 192, 'bounds': 192, 'gaps/clusters': 134, 'disc': 168}\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "scenes = get_scens_per_dim()\n",
    "per_label = {\"unif\":0, \"centre\":0, \"bounds\":0, \"gaps/clusters\":0, \"disc\":0}\n",
    "X = []\n",
    "y = []\n",
    "realY = []\n",
    "for scene in scenes:\n",
    "    label = scene[0]\n",
    "    realLabel = f\"{label} \" + json.dumps(scene[1])\n",
    "    kwargs = scene[1]\n",
    "    if (label == \"unif\"):\n",
    "        rep1 = 4 * rep\n",
    "    elif (label in [\"trunc_unif\", \"cauchy\", \"norm\"]):\n",
    "        rep1 = int(rep / 32)\n",
    "    elif (label in [\"bound_thing\",\"inv_norm\", \"inv_cauchy\"]):\n",
    "        rep1 = int(rep / 48)\n",
    "    elif (label in [\"clusters\",\"gaps\", \"part_unif\"]):\n",
    "        rep1 = int(rep / 67)\n",
    "    elif (label in [\"spikes\", \"shifted_spikes\"]):\n",
    "        rep1 = int(rep / 42)\n",
    "    data = get_simulated_data(label, rep=rep1, n_samples = n_samples, kwargs=kwargs)\n",
    "    for r in range(rep1):\n",
    "        X.append(np.sort(data[:,r]))\n",
    "    if (label in [\"trunc_unif\", \"cauchy\", \"norm\"]):\n",
    "        label = \"centre\"\n",
    "    elif (label in [\"bound_thing\",\"inv_norm\", \"inv_cauchy\"]):\n",
    "        label = \"bounds\"\n",
    "    elif (label in [\"gaps\", \"part_unif\", \"clusters\"]):\n",
    "        label = \"gaps/clusters\"\n",
    "    elif (label in [\"spikes\", \"shifted_spikes\"]):\n",
    "        label = \"disc\"\n",
    "    per_label[label] += rep1\n",
    "    y.extend([label]*rep1)\n",
    "    realY.extend([realLabel] * rep1)\n",
    "\n",
    "print(per_label)\n",
    "X = np.array(X)\n",
    "int_y, targetnames= pd.factorize(y)\n",
    "int_real_y, targetnames_real= pd.factorize(realY)\n",
    "\n",
    "cat_y = to_categorical(int_y)\n",
    "cat_y_real = to_categorical(int_real_y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, cat_y, test_size=0.2, random_state=42, stratify=int_y)\n",
    "\n",
    "#expand dims\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model from h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 20:51:24.790718: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-12-20 20:51:24.790775: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: neocortex\n",
      "2022-12-20 20:51:24.790783: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: neocortex\n",
      "2022-12-20 20:51:24.790912: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.42.1\n",
      "2022-12-20 20:51:24.790943: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.42.1\n",
      "2022-12-20 20:51:24.790950: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.42.1\n",
      "2022-12-20 20:51:24.791229: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 20ms/step - loss: 0.2118 - accuracy: 0.9765\n",
      "10/10 [==============================] - 0s 18ms/step\n",
      "Accuracy: [0.21176068484783173, 0.9765100479125977] f1 score: 0.9635308909276988\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model = tf.keras.models.load_model(f\"../BIAS/models/opt_cnn_model-{n_samples}.h5\")\n",
    "#model.summary()\n",
    "print(\n",
    "    \"Accuracy: {accuracy}\".format(\n",
    "        accuracy = model.evaluate(x=X_test, y=y_test)\n",
    "    ),\n",
    "    \"f1 score: {f1}\".format(\n",
    "        f1 = f1_score(np.argmax(y_test, axis=1), np.argmax(model.predict(X_test), axis=1), average='macro')\n",
    "    )\n",
    ")\n",
    "#tf.keras.utils.plot_model(model, to_file=f\"models/opt_cnn_model-{n_samples}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of wrongly predicted cases in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 18ms/step\n",
      "10/10 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "model1 = newmodel(model)\n",
    "hat_y = model1.predict(X_test)\n",
    "\n",
    "hat_y_real = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute shap values for all test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neocortex/repos/BIAS/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "keras is no longer supported, please use tf.keras instead.\n",
      "Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n"
     ]
    }
   ],
   "source": [
    "#using shap\n",
    "import seaborn as sbs\n",
    "import shap\n",
    "# select backgroud for shap\n",
    "background = X_train[np.random.choice(X_train.shape[0], 1000, replace=False)]\n",
    "# DeepExplainer to explain predictions of the model\n",
    "explainer = shap.DeepExplainer(model, background)\n",
    "# compute shap values\n",
    "\n",
    "#different try\n",
    "#bg = shap.maskers.Partition(shap.utils.sample(X,100))\n",
    "#explainer = shap.explainers.Partition(f, bg)\n",
    "\n",
    "#x_explained = X_test[:100]\n",
    "#shap_values = explainer.shap_values(x_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TFDeep.phi_symbolic.<locals>.grad_graph at 0x7f74145d51f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
      "41.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
      "38.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
      "63.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
      "38.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
      "63.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n"
     ]
    }
   ],
   "source": [
    "#plot shap explanations\n",
    "from matplotlib.gridspec import GridSpec\n",
    "def plot_explanation(x, preds, pred_labels, shap_vals_pred, shap_vals_true, prediction, label, filename):\n",
    "    plt.ioff()\n",
    "    cmap = sbs.color_palette('coolwarm', as_cmap=True)\n",
    "    norm = plt.Normalize(vmin=-1*np.max(np.abs(shap_vals_pred)), vmax=np.max(np.abs(shap_vals_pred)))  # 0 and 1 are the defaults, but you can adapt these to fit other uses\n",
    "    df = pd.DataFrame({\"x\": x.flatten(), \"shap\": shap_vals_pred.flatten()})\n",
    "    palette = {h: cmap(norm(h)) for h in df['shap']}\n",
    "\n",
    "    #fig, axs = plt.subplots(2, figsize=(8,2))\n",
    "    fig = plt.figure(figsize=(6,3))\n",
    "    gs = GridSpec(2, 3, figure=fig)\n",
    "    ax1 = fig.add_subplot(gs[:, 0]) #prediction labels\n",
    "    ax1.bar(pred_labels, preds)\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    ax1.set_title(\"Prediction probabilities\")\n",
    "    ax1.set_ylim([0,1])\n",
    "\n",
    "    ax2 = fig.add_subplot(gs[0, 1:])\n",
    "    ax2.set_title(f\"Predicted label: {prediction}\")\n",
    "    sbs.swarmplot(data=df, x=\"x\", hue=\"shap\", palette=palette, ax=ax2, size=3, legend=False)\n",
    "    ax2.set_xlabel(\"\")\n",
    "    ax2.set_xlim([0,1])\n",
    "\n",
    "    cmap = sbs.color_palette('coolwarm', as_cmap=True)\n",
    "    norm = plt.Normalize(vmin=-1*np.max(np.abs(shap_vals_true)), vmax=np.max(np.abs(shap_vals_true)))  # 0 and 1 are the defaults, but you can adapt these to fit other uses\n",
    "    df = pd.DataFrame({\"x\": x.flatten(), \"shap\": shap_vals_true.flatten()})\n",
    "    palette = {h: cmap(norm(h)) for h in df['shap']}\n",
    "    ax3 = fig.add_subplot(gs[1, 1:])\n",
    "    sbs.swarmplot(data=df, x=\"x\", hue=\"shap\", palette=palette, ax=ax3, size=3, legend=False)\n",
    "    ax3.set_title(f\"True label: {label}\")\n",
    "    ax3.set_xlabel(\"\")\n",
    "    ax3.set_xlim([0,1])\n",
    "    \n",
    "    #sbs.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "test_y = np.argmax(y_test, axis=1)\n",
    "for i in range(len(hat_y)):\n",
    "    if hat_y[i] != test_y[i]:\n",
    "        shap_val = explainer.shap_values(X_test[i:i+1])\n",
    "        plot_explanation(X_test[i], \n",
    "            hat_y_real[i], \n",
    "            targetnames,\n",
    "            shap_val[hat_y[i]][0], \n",
    "            shap_val[test_y[i]][0], \n",
    "            targetnames[hat_y[i]], \n",
    "            targetnames[test_y[i]], \n",
    "            f\"misclassifications/{n_samples}prediction{i}.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9/1000 [..............................] - ETA: 6s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "\n",
    "test_y = np.argmax(y_test, axis=1)\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "plot_confusion_matrix(model1, X_test, test_y, normalize='true', xticks_rotation = 'vertical', display_labels = targetnames, ax=ax) \n",
    "plt.savefig(f\"models/opt_cnn_model-{n_samples}-confusion.png\")\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c99927486eab2e85dcdb4aaad774b6b681d1900d0d2b9307a229dff4ffb6a2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
