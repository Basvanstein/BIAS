{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 14:53:33.180601: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-29 14:53:33.792390: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-29 14:53:33.809566: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-29 14:53:33.809653: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-29 14:53:35.793609: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-29 14:53:35.793738: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-29 14:53:35.793745: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import autokeras as ak\n",
    "#load data\n",
    "from BIAS.SB_Test_runner import get_scens_per_dim, get_simulated_data\n",
    "\n",
    "class newmodel(MLPClassifier):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def predict(self, X):\n",
    "        y = self.model.predict(X)\n",
    "        return np.argmax(y, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings for this experiment\n",
    "\n",
    "20000 repetitions per class (group of scenes) and 500 samples (runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings for this experiment\n",
    "rep = 20000\n",
    "n_samples = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unif': 80000, 'centre': 20000, 'bounds': 19968, 'gaps/clusters': 19966, 'disc': 19992}\n",
      "['unif {}' 'unif {}' 'unif {}' ... 'unif {}' 'unif {}'\n",
      " 'norm {\"sigma\": 0.1, \"mu\": 0.6}']\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "scenes = get_scens_per_dim()\n",
    "per_label = {\"unif\":0, \"centre\":0, \"bounds\":0, \"gaps/clusters\":0, \"disc\":0}\n",
    "X = []\n",
    "y = []\n",
    "realY = []\n",
    "for scene in scenes:\n",
    "    label = scene[0]\n",
    "    realLabel = f\"{label} \" + json.dumps(scene[1])\n",
    "    kwargs = scene[1]\n",
    "    if (label == \"unif\"):\n",
    "        rep1 = 4 * rep\n",
    "    elif (label in [\"trunc_unif\", \"cauchy\", \"norm\"]):\n",
    "        rep1 = int(rep / 32)\n",
    "    elif (label in [\"bound_thing\",\"inv_norm\", \"inv_cauchy\"]):\n",
    "        rep1 = int(rep / 48)\n",
    "    elif (label in [\"clusters\",\"gaps\", \"part_unif\"]):\n",
    "        rep1 = int(rep / 67)\n",
    "    elif (label in [\"spikes\", \"shifted_spikes\"]):\n",
    "        rep1 = int(rep / 42)\n",
    "    data = get_simulated_data(label, rep=rep1, n_samples = n_samples, kwargs=kwargs)\n",
    "    for r in range(rep1):\n",
    "        X.append(np.sort(data[:,r]))\n",
    "    if (label in [\"trunc_unif\", \"cauchy\", \"norm\"]):\n",
    "        label = \"centre\"\n",
    "    elif (label in [\"bound_thing\",\"inv_norm\", \"inv_cauchy\"]):\n",
    "        label = \"bounds\"\n",
    "    elif (label in [\"gaps\", \"part_unif\", \"clusters\"]):\n",
    "        label = \"gaps/clusters\"\n",
    "    elif (label in [\"spikes\", \"shifted_spikes\"]):\n",
    "        label = \"disc\"\n",
    "    per_label[label] += rep1\n",
    "    y.extend([label]*rep1)\n",
    "    realY.extend([realLabel] * rep1)\n",
    "\n",
    "print(per_label)\n",
    "X = np.array(X)\n",
    "int_y, targetnames= pd.factorize(y)\n",
    "int_real_y, targetnames_real= pd.factorize(realY)\n",
    "\n",
    "cat_y = to_categorical(int_y)\n",
    "cat_y_real = to_categorical(int_real_y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, cat_y, test_size=0.2, random_state=42, stratify=int_y)\n",
    "\n",
    "#expand dims\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model from h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 16:22:12.603765: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-11-29 16:22:12.693237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-QJMTJF6P): /proc/driver/nvidia/version does not exist\n",
      "2022-11-29 16:22:13.408399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model = tf.keras.models.load_model(f\"opt_cnn_model-{n_samples}.h5\")\n",
    "model.summary()\n",
    "print(\n",
    "    \"Accuracy: {accuracy}\".format(\n",
    "        accuracy = model.evaluate(x=X_test, y=y_test)\n",
    "    ),\n",
    "    \"f1 score: {f1}\".format(\n",
    "        f1 = f1_score(np.argmax(y_test, axis=1), np.argmax(model.predict(X_test), axis=1), average='macro')\n",
    "    )\n",
    ")\n",
    "tf.keras.utils.plot_model(model, to_file=f\"opt_cnn_model-{n_samples}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of wrongly predicted cases in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step\n",
      "1000/1000 [==============================] - 6s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "model1 = newmodel(model)\n",
    "hat_y = model1.predict(X_test)\n",
    "\n",
    "hat_y_real = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse\n",
    "import seaborn as sbs\n",
    "\n",
    "\n",
    "test_y = np.argmax(y_test, axis=1)\n",
    "for i in range(len(hat_y)):\n",
    "    if hat_y[i] != test_y[i]:\n",
    "        fig, axs = plt.subplots(1, figsize=(3,8))\n",
    "        sbs.swarmplot(data=X_test[i].flatten(), ax=axs, size=2)\n",
    "        plt.title(f\"Prediction: {targetnames[hat_y[i]]} \\n True label: {targetnames[test_y[i]]}\")\n",
    "        #plt.show()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"misclasifications/prediction{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute shap values for all test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "`tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    }
   ],
   "source": [
    "#using shap\n",
    "import shap\n",
    "# select backgroud for shap\n",
    "background = X_train[np.random.choice(X_train.shape[0], 1000, replace=False)]\n",
    "# DeepExplainer to explain predictions of the model\n",
    "explainer = shap.DeepExplainer(model, background)\n",
    "# compute shap values\n",
    "\n",
    "#different try\n",
    "#bg = shap.maskers.Partition(shap.utils.sample(X,100))\n",
    "#explainer = shap.explainers.Partition(f, bg)\n",
    "\n",
    "x_explained = X_test[:100]\n",
    "shap_values = explainer.shap_values(x_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 100, 500, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(shap_values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(hat_y)):\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m hat_y[i] \u001b[39m!=\u001b[39m test_y[i] \u001b[39mand\u001b[39;00m targetnames[test_y[i]] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39munif\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 43\u001b[0m         shap_val \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49mshap_values(X_test[i:i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m     44\u001b[0m         plot_explanation(X_test[i], \n\u001b[1;32m     45\u001b[0m             hat_y_real[i], \n\u001b[1;32m     46\u001b[0m             targetnames,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m             targetnames[test_y[i]], \n\u001b[1;32m     51\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmisclasifications-unif/prediction\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/BIAS/env/lib/python3.8/site-packages/shap/explainers/_deep/__init__.py:124\u001b[0m, in \u001b[0;36mDeep.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshap_values\u001b[39m(\u001b[39mself\u001b[39m, X, ranked_outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, output_rank_order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, check_additivity\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     91\u001b[0m     \u001b[39m\"\"\" Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m        were chosen as \"top\".\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplainer\u001b[39m.\u001b[39;49mshap_values(X, ranked_outputs, output_rank_order, check_additivity\u001b[39m=\u001b[39;49mcheck_additivity)\n",
      "File \u001b[0;32m~/repos/BIAS/env/lib/python3.8/site-packages/shap/explainers/_deep/deep_tf.py:312\u001b[0m, in \u001b[0;36mTFDeep.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39m# run attribution computation graph\u001b[39;00m\n\u001b[1;32m    311\u001b[0m feature_ind \u001b[39m=\u001b[39m model_output_ranks[j,i]\n\u001b[0;32m--> 312\u001b[0m sample_phis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mphi_symbolic(feature_ind), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_inputs, joint_input)\n\u001b[1;32m    314\u001b[0m \u001b[39m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X)):\n",
      "File \u001b[0;32m~/repos/BIAS/env/lib/python3.8/site-packages/shap/explainers/_deep/deep_tf.py:372\u001b[0m, in \u001b[0;36mTFDeep.run\u001b[0;34m(self, out, model_inputs, X)\u001b[0m\n\u001b[1;32m    369\u001b[0m         tf_execute\u001b[39m.\u001b[39mrecord_gradient \u001b[39m=\u001b[39m tf_backprop\u001b[39m.\u001b[39mrecord_gradient\n\u001b[1;32m    371\u001b[0m     \u001b[39mreturn\u001b[39;00m final_out\n\u001b[0;32m--> 372\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute_with_overridden_gradients(anon)\n",
      "File \u001b[0;32m~/repos/BIAS/env/lib/python3.8/site-packages/shap/explainers/_deep/deep_tf.py:408\u001b[0m, in \u001b[0;36mTFDeep.execute_with_overridden_gradients\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39m# define the computation graph for the attribution values using a custom gradient-like computation\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     out \u001b[39m=\u001b[39m f()\n\u001b[1;32m    409\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[39m# reinstate the backpropagatable check\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tf_gradients_impl, \u001b[39m\"\u001b[39m\u001b[39m_IsBackpropagatable\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/repos/BIAS/env/lib/python3.8/site-packages/shap/explainers/_deep/deep_tf.py:365\u001b[0m, in \u001b[0;36mTFDeep.run.<locals>.anon\u001b[0;34m()\u001b[0m\n\u001b[1;32m    363\u001b[0m     v \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant(data, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_inputs[i]\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    364\u001b[0m     inputs\u001b[39m.\u001b[39mappend(v)\n\u001b[0;32m--> 365\u001b[0m final_out \u001b[39m=\u001b[39m out(inputs)\n\u001b[1;32m    366\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     tf_execute\u001b[39m.\u001b[39mrecord_gradient \u001b[39m=\u001b[39m tf_backprop\u001b[39m.\u001b[39m_record_gradient\n",
      "File \u001b[0;32m~/repos/BIAS/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/repos/BIAS/env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/repos/BIAS/env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    917\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 919\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    920\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    921\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/BIAS/env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/repos/BIAS/env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/repos/BIAS/env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/repos/BIAS/env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#plot shap explanations\n",
    "from matplotlib.gridspec import GridSpec\n",
    "def plot_explanation(x, preds, pred_labels, shap_vals_pred, shap_vals_true, prediction, label, filename):\n",
    "    plt.ioff()\n",
    "    cmap = sbs.color_palette('coolwarm', as_cmap=True)\n",
    "    norm = plt.Normalize(vmin=-1*np.max(np.abs(shap_vals_pred)), vmax=np.max(np.abs(shap_vals_pred)))  # 0 and 1 are the defaults, but you can adapt these to fit other uses\n",
    "    df = pd.DataFrame({\"x\": x.flatten(), \"shap\": shap_vals_pred.flatten()})\n",
    "    palette = {h: cmap(norm(h)) for h in df['shap']}\n",
    "\n",
    "    #fig, axs = plt.subplots(2, figsize=(8,2))\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    gs = GridSpec(2, 3, figure=fig)\n",
    "    ax1 = fig.add_subplot(gs[:, 0]) #prediction labels\n",
    "    ax1.bar(pred_labels, preds)\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    ax1.set_title(\"Prediction probabilities\")\n",
    "    ax1.set_ylim([0,1])\n",
    "\n",
    "    ax2 = fig.add_subplot(gs[0, 1:])\n",
    "    ax2.set_title(f\"Predicted: {prediction}\")\n",
    "    sbs.swarmplot(data=df, x=\"x\", hue=\"shap\", palette=palette, ax=ax2, size=4, legend=False)\n",
    "    ax2.set_xlabel(\"\")\n",
    "    ax2.set_xlim([0,1])\n",
    "\n",
    "    cmap = sbs.color_palette('coolwarm', as_cmap=True)\n",
    "    norm = plt.Normalize(vmin=-1*np.max(np.abs(shap_vals_true)), vmax=np.max(np.abs(shap_vals_true)))  # 0 and 1 are the defaults, but you can adapt these to fit other uses\n",
    "    df = pd.DataFrame({\"x\": x.flatten(), \"shap\": shap_vals_true.flatten()})\n",
    "    palette = {h: cmap(norm(h)) for h in df['shap']}\n",
    "    ax3 = fig.add_subplot(gs[1, 1:])\n",
    "    sbs.swarmplot(data=df, x=\"x\", hue=\"shap\", palette=palette, ax=ax3, size=4, legend=False)\n",
    "    ax3.set_title(f\"Label: {label}\")\n",
    "    ax3.set_xlabel(\"\")\n",
    "    ax3.set_xlim([0,1])\n",
    "    \n",
    "    #sbs.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "test_y = np.argmax(y_test, axis=1)\n",
    "for i in range(len(hat_y)):\n",
    "    if hat_y[i] != test_y[i] and targetnames[test_y[i]] == \"unif\":\n",
    "        shap_val = explainer.shap_values(X_test[i:i+1])\n",
    "        plot_explanation(X_test[i], \n",
    "            hat_y_real[i], \n",
    "            targetnames,\n",
    "            shap_val[hat_y[i]][0], \n",
    "            shap_val[test_y[i]][0], \n",
    "            targetnames[hat_y[i]], \n",
    "            targetnames[test_y[i]], \n",
    "            f\"misclasifications-unif/prediction{i}.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9/1000 [..............................] - ETA: 6s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "plot_confusion_matrix(model1, X_test, test_y, normalize='true', xticks_rotation = 'vertical', display_labels = targetnames, ax=ax) \n",
    "plt.savefig(f\"opt_cnn_model-{n_samples}-confusion.png\")\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a76072e57cb99e830f08858204c12749db1ec5fb9144b5e7cedd260026f43c60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
